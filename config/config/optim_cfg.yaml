optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.01
lr_scheduler:
  class_path: torch.optim.lr_scheduler.ExponentialLR
  init_args:
    gamma: 0.1